{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDQ8sw6m7L1q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. 데이터 생성 (불균형한 다중 분류 데이터 예시)\n",
        "# ---------------------------------------------------------\n",
        "# 1000개의 샘플, 3개의 클래스, 불균형 비율 설정\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_classes=3, weights=[0.1, 0.3, 0.6], random_state=42)\n",
        "\n",
        "# 학습/테스트 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. 클래스 불균형 처리 (Sample Weight 계산)\n",
        "# ---------------------------------------------------------\n",
        "# 다중 분류에서는 scale_pos_weight 대신 각 샘플에 가중치를 부여합니다.\n",
        "# 희소한 클래스일수록 높은 가중치를 갖게 됩니다.\n",
        "sample_weights = compute_sample_weight(\n",
        "    class_weight='balanced',\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. 모델 및 GridSearchCV 설정\n",
        "# ---------------------------------------------------------\n",
        "# XGBClassifier 초기화\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='multi:softprob', # 다중 분류 확률\n",
        "    num_class=3,                # 클래스 개수\n",
        "    tree_method='hist',         # 속도 및 범주형 변수 처리에 유리\n",
        "    device='cpu',               # GPU 사용 시 'cuda'\n",
        "    enable_categorical=True,    # 범주형 데이터 지원 (필요 시)\n",
        "    eval_metric='mlogloss',\n",
        "    early_stopping_rounds = 50,# 다중 분류 평가 지표\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 튜닝할 파라미터 그리드\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_child_weight': [1, 3, 5], # 결측/노이즈가 많을 때 중요\n",
        "    'n_estimators': [1000]         # Early Stopping을 위해 충분히 크게 설정\n",
        "}\n",
        "\n",
        "# GridSearchCV 설정\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_macro', # 불균형 데이터이므로 Macro F1 사용 권장\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. 학습 실행 (Early Stopping 포함)\n",
        "# ---------------------------------------------------------\n",
        "print(\"GridSearchCV 수행 중...\")\n",
        "\n",
        "# fit 파라미터에 early_stopping 관련 설정 주입\n",
        "fit_params = {\n",
        "    'eval_set': [(X_train, y_train), (X_test, y_test)], # 학습 곡선을 위해 Train/Test 모두 전달\n",
        "    'sample_weight': sample_weights, # [중요] 불균형 처리를 위한 가중치 적용\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "grid_search.fit(X_train, y_train, **fit_params)\n",
        "\n",
        "# 최적의 모델 추출\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"\\n최적 파라미터: {grid_search.best_params_}\")\n",
        "print(f\"최고 CV 점수 (Macro F1): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. 최종 평가\n",
        "# ---------------------------------------------------------\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"\\n[Classification Report]\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. 학습 곡선 (Learning Curve) 시각화\n",
        "# ---------------------------------------------------------\n",
        "results = best_model.evals_result()\n",
        "epochs = len(results['validation_0']['mlogloss'])\n",
        "x_axis = range(0, epochs)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Log Loss 차트\n",
        "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
        "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test (Validation)')\n",
        "ax.legend()\n",
        "plt.ylabel('Log Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('XGBoost Learning Curve (Log Loss)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 데이터 생성 (가상의 \"지저분한\" 데이터 만들기)\n",
        "# =============================================================================\n",
        "# 2000개의 샘플, 심각한 불균형 (Class 0: 5%, Class 1: 15%, Class 2: 80%)\n",
        "X, y = make_classification(n_samples=2000, n_features=15, n_informative=10,\n",
        "                           n_classes=3, weights=[0.05, 0.15, 0.8], random_state=42)\n",
        "\n",
        "# DataFrame 변환\n",
        "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "df_X = pd.DataFrame(X, columns=feature_names)\n",
        "df_y = pd.Series(y)\n",
        "\n",
        "# [시나리오 적용] 1. 결측치(NaN) 강제 주입 (전체 데이터의 20%)\n",
        "for col in df_X.columns:\n",
        "    df_X.loc[df_X.sample(frac=0.2).index, col] = np.nan\n",
        "\n",
        "# [시나리오 적용] 2. 범주형 변수 변환 (마지막 3개 컬럼을 범주형으로 가정)\n",
        "cat_cols = feature_names[-3:]\n",
        "for col in cat_cols:\n",
        "    # 임의로 구간을 나누어 범주화 (예: 'A', 'B', 'C')\n",
        "    df_X[col] = pd.cut(df_X[col], bins=3, labels=['A', 'B', 'C'])\n",
        "    # ★ 중요: XGBoost Native Support를 위해 'category' 타입으로 변환\n",
        "    df_X[col] = df_X[col].astype(\"category\")\n",
        "\n",
        "print(\">>> 데이터 준비 완료\")\n",
        "print(f\"결측치 개수: {df_X.isna().sum().sum()}\")\n",
        "print(f\"데이터 타입 확인:\\n{df_X.dtypes.tail(4)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 학습/검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42, stratify=df_y)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 강력한 클래스 가중치 (Sample Weight) 생성\n",
        "# =============================================================================\n",
        "# Class 0(5%)을 놓치지 않기 위해 가중치를 아주 강하게 설정\n",
        "# 기본 balanced 가중치보다 Class 0에 더 가산점을 줌\n",
        "custom_class_weights = {\n",
        "    0: 20.0,  # 소수 클래스 (매우 중요)\n",
        "    1: 3.0,   # 중간 클래스\n",
        "    2: 1.0    # 다수 클래스\n",
        "}\n",
        "\n",
        "# 설정한 비율대로 각 샘플마다 가중치 부여\n",
        "train_sample_weights = compute_sample_weight(\n",
        "    class_weight=custom_class_weights,\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "print(\">>> 클래스별 부여된 가중치 예시:\")\n",
        "for c in [0, 1, 2]:\n",
        "    idx = np.where(y_train == c)[0][0]\n",
        "    print(f\"Class {c}: {train_sample_weights[idx]:.1f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 모델 및 GridSearchCV 설정\n",
        "# =============================================================================\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=3,\n",
        "    tree_method='hist',       # [핵심] 결측치/범주형 처리에 필수\n",
        "    enable_categorical=True,  # [핵심] 범주형 변수 자동 처리\n",
        "    device='cpu',\n",
        "    eval_metric='mlogloss',\n",
        "    early_stopping_rounds = 50,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 결측치가 많고 노이즈가 많은 데이터에 맞춘 파라미터 그리드\n",
        "param_grid = {\n",
        "    'n_estimators': [1000],          # Early Stopping용 (충분히 크게)\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5],             # 깊이를 제한하여 과적합 방지\n",
        "    'min_child_weight': [5, 10],     # [핵심] 결측치 노이즈 방지를 위해 높게 설정\n",
        "    'colsample_bytree': [0.7],       # 변수 샘플링\n",
        "    'reg_alpha': [1.0, 5.0]          # [핵심] L1 정규화로 불필요한 변수 제거 효과\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_macro',              # 불균형 데이터이므로 Macro F1 사용\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 학습 실행 (Early Stopping 포함)\n",
        "# =============================================================================\n",
        "# fit_params에 sample_weight와 eval_set을 모두 전달\n",
        "fit_params = {\n",
        "    'eval_set': [(X_train, y_train), (X_test, y_test)],\n",
        "    'sample_weight': train_sample_weights, # ★ 가중치 적용\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "print(\">>> GridSearchCV 학습 시작...\")\n",
        "grid_search.fit(X_train, y_train, **fit_params)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"\\n>>> 최적 파라미터: {grid_search.best_params_}\")\n",
        "print(f\">>> 최고 CV 점수 (Macro F1): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 최종 평가 및 시각화\n",
        "# =============================================================================\n",
        "# 예측\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# [결과 1] Classification Report\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Classification Report\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# [결과 2] Confusion Matrix 시각화\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
        "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "# [결과 3] 학습 곡선 (Learning Curve) 시각화\n",
        "results = best_model.evals_result()\n",
        "epochs = len(results['validation_0']['mlogloss'])\n",
        "x_axis = range(0, epochs)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
        "plt.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
        "plt.legend()\n",
        "plt.ylabel('Log Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('XGBoost Learning Curve (Log Loss)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Z_C7QrV7PIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}